---
title: "AZ Task 3"
author: "AJ Zekanoski"
date: "2/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
library(textdata)
library(pdftools)
library(ggwordcloud)
```

Read in cowboy songs text.
```{r, cache = TRUE}
cowboys_text <- pdf_text("cowboys.pdf")
```

Check to see one of the pages.
```{r}
cowboys34 <- cowboys_text[34]

cowboys34
```

Change it to a tidy dataframe.
```{r}
cowboy_tidy <- data.frame(cowboys_text) %>% 
  mutate(text_full = str_split(cowboys_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full))
```

```{r}
cowboy_df <- cowboy_tidy %>% 
  slice(-(1:370))
```

Give a new row for each word. 
```{r}
cowboy_tokens <- cowboy_df %>% 
  unnest_tokens(word, text_full) %>% 
  dplyr::select(-cowboys_text)
```

Count all the words up and eliminate all the numbers in the beginning. 
```{r}
cowboy_wordcount <- cowboy_tokens %>% 
  count(word) %>% 
  slice(-(1:155))
```

Time to remove stop words. 
```{r}
cowboy_nonstop_words <- cowboy_tokens %>% 
  anti_join(stop_words)
```

Recount them and get rid of the page numbers
```{r}
nonstop_counts <- cowboy_nonstop_words %>% 
  count(word) %>% 
  slice(-(1:154))
```

Let's look at the top 100 words used in Songs of the Cattle Trail and Cow Camp.
```{r}
top_100_words <- nonstop_counts %>% 
  arrange(-n) %>% 
  slice(1:100) 
```

Some cowboy specific stop words snuck through to this step so we're going to manually eliminate them. They are: til, em, fer, yer, git, o'er, yo, thar, and twas. Leaves us with the 91 most used non-stop words.

```{r}
top_words <- top_100_words %>% 
  slice(-c(4,5,6,16,43,45,49,73,88))
```

Let's make a word cloud for Songs of the Cattle Trail and Cow Camp
```{r}

```

